{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10805434,"sourceType":"datasetVersion","datasetId":6706986}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment: Image recognition\n- Alumno 1: Dakota Mellish\n- Alumno 2: Javier Arteaga\n- Alumno 3: Rodrigo Castanon\n\nThe goals of the assignment are:\n* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n\nFollow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/4oNHlRFEd71HXp4](https://drive.upm.es/s/4oNHlRFEd71HXp4)","metadata":{"editable":true,"id":"QYuALZOG-AMq","slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"code","source":"\n\n     \nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.031186Z","start_time":"2024-10-26T00:00:17.131476Z"},"editable":true,"execution":{"iopub.status.busy":"2025-02-27T10:59:54.954460Z","iopub.execute_input":"2025-02-27T10:59:54.954712Z","iopub.status.idle":"2025-02-27T11:00:07.294137Z","shell.execute_reply.started":"2025-02-27T10:59:54.954682Z","shell.execute_reply":"2025-02-27T11:00:07.293363Z"},"slideshow":{"slide_type":""},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Set Working Directory","metadata":{}},{"cell_type":"code","source":"import os \n\nprint(os.getcwd())\n\nos.chdir('/kaggle')\nprint(os.getcwd())\n\n\nbase_dir='/kaggle/input/xview-recognition/xview_recognition/'","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:07.295401Z","iopub.execute_input":"2025-02-27T11:00:07.295841Z","iopub.status.idle":"2025-02-27T11:00:07.300742Z","shell.execute_reply.started":"2025-02-27T11:00:07.295820Z","shell.execute_reply":"2025-02-27T11:00:07.299920Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\n/kaggle\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import uuid\nimport numpy as np\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.066937Z","start_time":"2024-10-26T00:00:21.059126Z"},"editable":true,"execution":{"iopub.status.busy":"2025-02-27T11:00:07.302143Z","iopub.execute_input":"2025-02-27T11:00:07.302487Z","iopub.status.idle":"2025-02-27T11:00:07.349099Z","shell.execute_reply.started":"2025-02-27T11:00:07.302457Z","shell.execute_reply":"2025-02-27T11:00:07.348434Z"},"id":"OYtqD3Oh-AMw","slideshow":{"slide_type":""},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"categories = {0: 'Cargo plane', 1: 'Helicopter', 2: 'Small car', 3: 'Bus', 4: 'Truck', 5: 'Motorboat', 6: 'Fishing vessel', 7: 'Dump truck', 8: 'Excavator', 9: 'Building', 10: 'Storage tank', 11: 'Shipping container'}","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.153693Z","start_time":"2024-10-26T00:00:21.149079Z"},"execution":{"iopub.status.busy":"2025-02-27T11:00:07.349942Z","iopub.execute_input":"2025-02-27T11:00:07.350219Z","iopub.status.idle":"2025-02-27T11:00:07.364010Z","shell.execute_reply.started":"2025-02-27T11:00:07.350191Z","shell.execute_reply":"2025-02-27T11:00:07.363333Z"},"id":"I_GygShu-AMz","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install rasterio\nimport warnings\nimport rasterio\nimport numpy as np\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open(base_dir+filename, 'r')\n    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.292654Z","start_time":"2024-10-26T00:00:21.205321Z"},"editable":true,"execution":{"iopub.status.busy":"2025-02-27T11:00:07.364633Z","iopub.execute_input":"2025-02-27T11:00:07.364812Z","iopub.status.idle":"2025-02-27T11:00:13.486290Z","shell.execute_reply.started":"2025-02-27T11:00:07.364796Z","shell.execute_reply":"2025-02-27T11:00:13.485597Z"},"id":"fRBA7ReQ-AM0","slideshow":{"slide_type":""},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Collecting rasterio\n  Downloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\nCollecting affine (from rasterio)\n  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (25.1.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2025.1.31)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\nDownloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nInstalling collected packages: affine, rasterio\nSuccessfully installed affine-2.4.0 rasterio-1.4.3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"#### Training\nDesign and train a ffNN to deal with the “xview_recognition” classification task.","metadata":{"id":"diNBB3qy-AM2"}},{"cell_type":"code","source":"\n\nimport json\n# Load database\njson_file = base_dir+'/xview_ann_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:21.416449Z","start_time":"2024-10-26T00:00:21.311510Z"},"editable":true,"execution":{"iopub.status.busy":"2025-02-27T11:00:13.488634Z","iopub.execute_input":"2025-02-27T11:00:13.489046Z","iopub.status.idle":"2025-02-27T11:00:13.645839Z","shell.execute_reply.started":"2025-02-27T11:00:13.489025Z","shell.execute_reply":"2025-02-27T11:00:13.645182Z"},"id":"Orto292C-AM3","slideshow":{"slide_type":""},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    obj = GenericObject()\n    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n    obj.category = json_ann['category_id']\n    # Resampling strategy to reduce training time\n    counts[obj.category] += 1\n    image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:22.874518Z","start_time":"2024-10-26T00:00:22.204948Z"},"execution":{"iopub.status.busy":"2025-02-27T11:00:13.647325Z","iopub.execute_input":"2025-02-27T11:00:13.647640Z","iopub.status.idle":"2025-02-27T11:00:14.046647Z","shell.execute_reply.started":"2025-02-27T11:00:13.647596Z","shell.execute_reply":"2025-02-27T11:00:14.045593Z"},"id":"4GjFLHs4-AM4","outputId":"5581df22-d4e9-42ac-9f94-061fd8c7acd9","trusted":true},"outputs":[{"name":"stdout","text":"{'Cargo plane': 635, 'Helicopter': 70, 'Small car': 4290, 'Bus': 2155, 'Truck': 2746, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 4689, 'Storage tank': 1469, 'Shipping container': 1523}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nanns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\nprint('Number of training images: ' + str(len(anns_train)))\nprint('Number of validation images: ' + str(len(anns_valid)))","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:23.656800Z","start_time":"2024-10-26T00:00:23.123245Z"},"execution":{"iopub.status.busy":"2025-02-27T11:00:14.047524Z","iopub.execute_input":"2025-02-27T11:00:14.047833Z","iopub.status.idle":"2025-02-27T11:00:14.445101Z","shell.execute_reply.started":"2025-02-27T11:00:14.047803Z","shell.execute_reply":"2025-02-27T11:00:14.444368Z"},"id":"NriAECvS-AM6","trusted":true},"outputs":[{"name":"stdout","text":"Number of training images: 19239\nNumber of validation images: 2138\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Model Create\nDesign model structure and shape ","metadata":{}},{"cell_type":"code","source":"# Load architecture\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, add,Dropout, Activation, Flatten,LeakyReLU,BatchNormalization\nfrom tensorflow.keras.regularizers import l1_l2\n\n\nfrom tensorflow.keras import Input\n\n\nfrom numpy import random\n\n\n\n## By default, the number of neurons is the same as the number of categories, scale down red \ndef make_model(n_layers_dense,n_neurons,categories,input_shape,scale_down=1,activation_list=None,dropout=None,batch_normalization=False,l1_normalization=None,l2_normalization=None):\n    \"\"\"\"\n    n_layers_dense (integer): number of non-output or flatten layers\n    n_neurons (integer): total number of neurons to use\n    categories (list): list of distinct values that the model will attempt to classify\n    input_shape (3-tuple): tuple indicating the dimensions of the input data\n    scale_down (float, default=1): value which can be used to scale down deeper layers (ex: scale_down.5 could be 1048-524-256)\n    activation_list (list): An in-order list which provides the activation functions to be used\n    dropout (float default None): An optional parameter to allow for dropout based on the specified amount\n    batch_normalization (boolean default False): An optional parameter to allow for batch normalization\n    l1_normalization (float default none): An optional parameter to allow for l1_kernel normalization\n    l2_normalization (float default none): An optional parameter to allow for l2_kernel normaliation\n    \"\"\"\n    \n    \n    model = Sequential()  \n    model.add(Flatten(input_shape=input_shape))\n    ## Pull common activation types from https://faroit.com/keras-docs/1.2.2/activations/\n    activation_types=['relu','sigmoid','softmax','tanh','softplus','softsign','sigmoid','linear']   \n    if scale_down is None:\n           scale_down=1\n    scale=scale_down\n    for layer in range(0,n_layers_dense):\n     \n\n        if l1_normalization or l2_normalization is not None:\n            if l1_normalization and l2_normalization is not None:\n                model.add(Dense(n_neurons*scale,kernel_regularization=l1_l2(l1_normalization,l2_normalization)))\n                model.add(Activation(activation_list([layer])))\n            elif l1_normalization is not None and l2_normalization is None:\n                model.add(Dense(n_neurons*scale,kernel_regularization=l1_l2(l1_normalization,0)))\n                model.add(Activation(activation_list([layer])))\n\n            elif l2_normalization is not None and l1_normalization is None:\n                model.add(Dense(n_neurons*scale,kernel_regularization=l1_l2(0,l2_normalization)))\n                model.add(Activation(activation_list([layer])))\n                \n                \n        if dropout is not None:\n                model.add(Dropout(dropout))\n        if batch_normalization==True:\n                model.add(BatchNormalization())\n                \n        scale=scale*scale_down\n    \n    ## Add final layer for output\n    model.add(Dense(len(categories),activation='softmax'))\n    return model\n\n\n\n\nmodel_1=make_model(n_layers_dense=3,n_neurons=1048,scale_down=.5,input_shape=(224,224,3),activation_list=['relu','relu','relu'],categories=categories,dropout=.5)\nmodel_2=make_model(n_layers_dense=3,n_neurons=1048,scale_down=.5,input_shape=(224,224,3),activation_list=['relu','relu','relu'],categories=categories,dropout=.3)\nmodel_3=make_model(n_layers_dense=3,n_neurons=1048,scale_down=.5,input_shape=(224,224,3),activation_list=['relu','relu','relu'],categories=categories,dropout=.2)\nmodel_4=make_model(n_layers_dense=3,n_neurons=1048,scale_down=.5,input_shape=(224,224,3),activation_list=['relu','relu','relu'],categories=categories,dropout=.1)\n\n\n# model.summary()\n\n\n\n\n\n\n\n### Here we can experiment with the architecture and try different amounts of layers\n\n# print('Load model')\n# model = Sequential()\n# ## This part will never change due to the input structure\n# model.add(Flatten(input_shape=(224, 224, 3)))\n\n# ## Here we can experiment between \n# model.add(Activation('relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(len(categories)))\n# model.add(Activation('softmax'))\n# model.summary()","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:25.056806Z","start_time":"2024-10-26T00:00:24.261581Z"},"execution":{"iopub.status.busy":"2025-02-27T11:00:53.029483Z","iopub.execute_input":"2025-02-27T11:00:53.029816Z","iopub.status.idle":"2025-02-27T11:00:54.634878Z","shell.execute_reply.started":"2025-02-27T11:00:53.029780Z","shell.execute_reply":"2025-02-27T11:00:54.634235Z"},"id":"BNkjbY2e-AM7","outputId":"47bde031-306f-464e-8e22-cc70a7fb7c67","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n\nlearning_rates=[1e-3,1e-2,1e-1]\n\n# Learning rate is changed to 0.001\nopt_1 = Adam(learning_rate=learning_rates[1], beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\nopt_2 = Adam(learning_rate=learning_rates[1], beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\nopt_3 = Adam(learning_rate=learning_rates[1], beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\nopt_4 = Adam(learning_rate=learning_rates[1], beta_1=0.9, beta_2=0.999, epsilon=1e-8, amsgrad=True, clipnorm=1.0)\n\nopt_2=SGD(learning_rate=learning_rates[1])\nopt_3=RMSprop(learning_rate=learning_rates[1])\n\n\n\nmodel_1.compile(optimizer=opt_1, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_2.compile(optimizer=opt_2, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_3.compile(optimizer=opt_3, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_4.compile(optimizer=opt_4, loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n\n## Here we can experiment with a different optimizer such as Momentum, SGD, or RMSPROP as well as learning rate\n\n\n","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:25.467525Z","start_time":"2024-10-26T00:00:25.434068Z"},"execution":{"iopub.status.busy":"2025-02-27T11:01:04.919962Z","iopub.execute_input":"2025-02-27T11:01:04.920274Z","iopub.status.idle":"2025-02-27T11:01:04.941550Z","shell.execute_reply.started":"2025-02-27T11:01:04.920249Z","shell.execute_reply":"2025-02-27T11:01:04.940821Z"},"id":"-aSlKtG6-AM7","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Callbacks\nmodel_checkpoint_1 = ModelCheckpoint('/kaggle/working/model_1.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\nmodel_checkpoint_2 = ModelCheckpoint('/kaggle/working/model_2.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\nmodel_checkpoint_3 = ModelCheckpoint('/kaggle/working/model_3.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\nmodel_checkpoint_4 = ModelCheckpoint('/kaggle/working/model_4.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\n\nreduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks_1 = [model_checkpoint_1, reduce_lr, early_stop, terminate]\ncallbacks_2 = [model_checkpoint_2, reduce_lr, early_stop, terminate]\ncallbacks_3 = [model_checkpoint_3, reduce_lr, early_stop, terminate]\ncallbacks_4 = [model_checkpoint_4, reduce_lr, early_stop, terminate]","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:26.254555Z","start_time":"2024-10-26T00:00:26.243908Z"},"execution":{"iopub.status.busy":"2025-02-27T11:01:07.007120Z","iopub.execute_input":"2025-02-27T11:01:07.007491Z","iopub.status.idle":"2025-02-27T11:01:07.025361Z","shell.execute_reply.started":"2025-02-27T11:01:07.007460Z","shell.execute_reply":"2025-02-27T11:01:07.024318Z"},"id":"GGAJEfpB-AM8","trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-a227783932ec>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mterminate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTerminateOnNaN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcallbacks_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcallbacks_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcallbacks_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcallbacks_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_checkpoint_2' is not defined"],"ename":"NameError","evalue":"name 'model_checkpoint_2' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"def generator_images(objs, batch_size, do_shuffle=False):\n    while True:\n        if do_shuffle:\n            np.random.shuffle(objs)\n        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n        for group in groups:\n            images, labels = [], []\n            for (filename, obj) in group:\n                # Load image\n                images.append(load_geoimage(filename))\n                probabilities = np.zeros(len(categories))\n                probabilities[list(categories.values()).index(obj.category)] = 1\n                labels.append(probabilities)\n            images = np.array(images).astype(np.float32)\n            labels = np.array(labels).astype(np.float32)\n            yield images, labels","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:01:13.360413Z","iopub.execute_input":"2025-02-27T11:01:13.360837Z","iopub.status.idle":"2025-02-27T11:01:13.368148Z","shell.execute_reply.started":"2025-02-27T11:01:13.360804Z","shell.execute_reply":"2025-02-27T11:01:13.367217Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Generate the list of objects from annotations\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n# Generators\nbatch_size = 16\ntrain_generator = generator_images(objs_train, batch_size, do_shuffle=True)\nvalid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)","metadata":{"ExecuteTime":{"end_time":"2024-10-26T00:00:27.058834Z","start_time":"2024-10-26T00:00:27.022627Z"},"execution":{"iopub.status.busy":"2025-02-27T11:01:15.004559Z","iopub.execute_input":"2025-02-27T11:01:15.004846Z","iopub.status.idle":"2025-02-27T11:01:15.029696Z","shell.execute_reply.started":"2025-02-27T11:01:15.004824Z","shell.execute_reply":"2025-02-27T11:01:15.028772Z"},"id":"Yht-QqUH-AM8","trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import math\nimport numpy as np\n\nprint('Training model')\nepochs = 20\ntrain_steps = math.ceil(len(objs_train)/batch_size)\nvalid_steps = math.ceil(len(objs_valid)/batch_size)\nh_1 = model_1.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks_1, verbose=1)\nh_2= model_2.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks_2, verbose=1)\nh_3=model_3.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks_3, verbose=1)\nh_4=model_4.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks_4, verbose=1)\n\n# # Best validation model\n# best_idx = int(np.argmax(h.history['val_accuracy']))\n# best_value = np.max(h.history['val_accuracy'])\n# print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))","metadata":{"ExecuteTime":{"start_time":"2024-10-26T00:00:27.913670Z"},"editable":true,"execution":{"iopub.status.busy":"2025-02-27T11:01:16.490067Z","iopub.execute_input":"2025-02-27T11:01:16.490411Z"},"id":"TrfpdECs-AM9","jupyter":{"is_executing":true},"outputId":"21d89b78-d94c-442e-9bc2-517654c0b614","slideshow":{"slide_type":""},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Training model\nEpoch 1/20\n\u001b[1m  49/1203\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:44\u001b[0m 506ms/step - accuracy: 0.1326 - loss: 36810.8516","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"#### Validation\nCompute validation metrics.","metadata":{"editable":true,"id":"8IMMO_mT-AM9","slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show(fig)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:14.832602Z","iopub.status.idle":"2025-02-27T11:00:14.832973Z","shell.execute_reply":"2025-02-27T11:00:14.832796Z"},"id":"HAanJ-V0-AM1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ncounter=0\nfor a_model in [model_1,model_2,model_3,model_4]:\n    model=a_model\n    num=counter+1\n    model.load_weights('/kaggle/working/model_'+str(num)+'.keras')\n    y_true, y_pred = [], []\n    for ann in anns_valid:\n        # Load image\n        image = load_geoimage(ann.filename)\n        for obj_pred in ann.objects:\n            # Generate prediction\n            warped_image = np.expand_dims(image, 0)\n            predictions = model.predict(warped_image, verbose=0)\n            # Save prediction\n            pred_category = list(categories.values())[np.argmax(predictions)]\n            pred_score = np.max(predictions)\n            y_true.append(obj_pred.category)\n            y_pred.append(pred_category)\n    from sklearn.metrics import confusion_matrix\n    \n    # Compute the confusion matrix\n    cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n    draw_confusion_matrix(cm, categories)\n\n    import numpy as np\n    \n    # Compute the accuracy\n    correct_samples_class = np.diag(cm).astype(float)\n    total_samples_class = np.sum(cm, axis=1).astype(float)\n    total_predicts_class = np.sum(cm, axis=0).astype(float)\n    print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n    acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n    print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n    acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n    print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n    for idx in range(len(categories)):\n        # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n        # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n        tp = cm[idx, idx]\n        fp = sum(cm[:, idx]) - tp\n        fn = sum(cm[idx, :]) - tp\n        tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n        # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n        recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n        # Precision: proportion of predicted positive cases that were truly real positives.\n        precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n        # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n        specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n        # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n        # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n        f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n        print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))\n    \n    import os\n    import numpy as np\n    \n    anns = []\n    for (dirpath, dirnames, filenames) in os.walk(base_dir+'xview_test'):\n        for filename in filenames:\n            image = GenericImage('xview_test/'+filename)\n            image.tile = np.array([0, 0, 224, 224])\n            obj = GenericObject()\n            obj.bb = (0, 0, 224, 224)\n            obj.category = dirpath[dirpath.rfind('/')+1:]\n            image.add_object(obj)\n            anns.append(image)\n    print('Number of testing images: ' + str(len(anns)))\n    \n    import numpy as np\n    \n    model.load_weights('/kaggle/working/model_'+str(num)+'.keras')\n    predictions_data = {\"images\": {}, \"annotations\": {}}\n    for idx, ann in enumerate(anns):\n        image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n        predictions_data[\"images\"][idx] = image_data\n        # Load image\n        image = load_geoimage(ann.filename)\n        for obj_pred in ann.objects:\n            # Generate prediction\n            warped_image = np.expand_dims(image, 0)\n            predictions = model.predict(warped_image, verbose=0)\n            # Save prediction\n            pred_category = list(categories.values())[np.argmax(predictions)]\n            pred_score = np.max(predictions)\n            annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n            predictions_data[\"annotations\"][idx] = annotation_data\n    \n    with open(\"/kaggle/working/prediction_model\"+str(counter)+\".json\", \"w\") as outfile:\n        json.dump(predictions_data, outfile)\n\n    counter+=1","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:14.834030Z","iopub.status.idle":"2025-02-27T11:00:14.834409Z","shell.execute_reply":"2025-02-27T11:00:14.834248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n\n# # Compute the accuracy\n# correct_samples_class = np.diag(cm).astype(float)\n# total_samples_class = np.sum(cm, axis=1).astype(float)\n# total_predicts_class = np.sum(cm, axis=0).astype(float)\n# print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n# acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n# print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n# acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n# print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n# for idx in range(len(categories)):\n#     # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n#     # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n#     tp = cm[idx, idx]\n#     fp = sum(cm[:, idx]) - tp\n#     fn = sum(cm[idx, :]) - tp\n#     tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n#     # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n#     recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n#     # Precision: proportion of predicted positive cases that were truly real positives.\n#     precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n#     # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n#     specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n#     # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n#     # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n#     f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n#     print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:14.835348Z","iopub.status.idle":"2025-02-27T11:00:14.835705Z","shell.execute_reply":"2025-02-27T11:00:14.835546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing\nTry to improve the results provided in the competition.","metadata":{}},{"cell_type":"code","source":"# import os\n# import numpy as np\n\n# anns = []\n# for (dirpath, dirnames, filenames) in os.walk(base_dir+'xview_test'):\n#     for filename in filenames:\n#         image = GenericImage('xview_test/'+filename)\n#         image.tile = np.array([0, 0, 224, 224])\n#         obj = GenericObject()\n#         obj.bb = (0, 0, 224, 224)\n#         obj.category = dirpath[dirpath.rfind('/')+1:]\n#         image.add_object(obj)\n#         anns.append(image)\n# print('Number of testing images: ' + str(len(anns)))\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:14.837756Z","iopub.status.idle":"2025-02-27T11:00:14.838074Z","shell.execute_reply":"2025-02-27T11:00:14.837923Z"},"id":"tJr_-xCt-AM-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n\n# model.load_weights('/kaggle/working/model.keras')\n# predictions_data = {\"images\": {}, \"annotations\": {}}\n# for idx, ann in enumerate(anns):\n#     image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n#     predictions_data[\"images\"][idx] = image_data\n#     # Load image\n#     image = load_geoimage(ann.filename)\n#     for obj_pred in ann.objects:\n#         # Generate prediction\n#         warped_image = np.expand_dims(image, 0)\n#         predictions = model.predict(warped_image, verbose=0)\n#         # Save prediction\n#         pred_category = list(categories.values())[np.argmax(predictions)]\n#         pred_score = np.max(predictions)\n#         annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n#         predictions_data[\"annotations\"][idx] = annotation_data","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:14.838987Z","iopub.status.idle":"2025-02-27T11:00:14.839318Z","shell.execute_reply":"2025-02-27T11:00:14.839171Z"},"id":"TGs2zqfv-AM_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with open(\"/kaggle/working/prediction.json\", \"w\") as outfile:\n#     json.dump(predictions_data, outfile)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T11:00:14.840177Z","iopub.status.idle":"2025-02-27T11:00:14.840507Z","shell.execute_reply":"2025-02-27T11:00:14.840390Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}